{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnQ6g6BWldYp",
        "colab_type": "code",
        "outputId": "d93c2672-e7f9-43ab-aadd-97d2afd05a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymri1Cpfy2e4",
        "colab_type": "text"
      },
      "source": [
        "*Weighted DC-Kmeans model:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-q_k37Iy2tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Kmeans(object):\n",
        "    '''\n",
        "    In-house implementation of k-means via Lloyd-Max iterations\n",
        "    This is a research prototype and is not necessarily well-optimized\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                    k,\n",
        "                    termination='fixed',\n",
        "                    iters=10,\n",
        "                    tol=10**-3):\n",
        "        '''\n",
        "        Constructor\n",
        "        INPUT:\n",
        "        k - # of centroids/clusters\n",
        "        iters - # of iterations to run\n",
        "        termination - {'fixed', 'loss', 'centers'}\n",
        "            if 'fixed' - runs for fixed # of iterations\n",
        "            if 'loss' - runs until loss converges\n",
        "            if 'centers' -runs until centers converge\n",
        "        tol - numeric tolerance to determine convergence\n",
        "        '''\n",
        "        # set parameters\n",
        "        self.k = k\n",
        "        self.iters = iters\n",
        "        self.tol = tol\n",
        "        self.termination = termination\n",
        "        # initialize placeholder values\n",
        "        self._init_placeholders()\n",
        "        self.mass_arr = None\n",
        "\n",
        "    def run(self, X):\n",
        "        '''\n",
        "        Run clustering algorithm\n",
        "        INPUT:\n",
        "        X - numpy matrix, n-by-d, each row is a data point\n",
        "        OUTPUT: (3-tuple)\n",
        "        centroids - k-by-d matrix of centroids\n",
        "        assignments - Vector of length n, with datapoint to center assignments\n",
        "        loss - The loss of the final partition\n",
        "        '''\n",
        "        self._set_data(X)\n",
        "        self._lloyd_iterations()\n",
        "        return self.centroids, self.assignments, self.loss\n",
        "\n",
        "    def delete(self, del_idx):\n",
        "        '''\n",
        "        Delete point associated with key del_idx\n",
        "        NOTE: del_idx must be int in {0,n-1}\n",
        "            After deleting any key other than n-1,\n",
        "            the (n-1)-th datapoint's key is automatically\n",
        "            swapped with del_idx to\n",
        "        '''\n",
        "        self.data = np.delete(self.data, del_idx, 0)\n",
        "        self.n = self.n-1\n",
        "        self._init_placeholders()\n",
        "        return self.run(self.data)\n",
        "\n",
        "    def _init_placeholders(self):\n",
        "        self.loss = np.Infinity\n",
        "        self.empty_clusters = []\n",
        "        self.kpp_inits = set()\n",
        "        self.centroids = None\n",
        "        self.assignments = None\n",
        "        self.model = None\n",
        "\n",
        "    def _set_data(self, X):\n",
        "        self.data = X\n",
        "        self.n, self.d = X.shape\n",
        "\n",
        "    def _lloyd_iterations(self):\n",
        "        self._init_centroids()\n",
        "        for _ in range(self.iters):\n",
        "            loss_prev = self.loss\n",
        "            centers_prev = self.model\n",
        "            self._assign_clusters()\n",
        "            self._assign_centroids()\n",
        "            prev = loss_prev if self.termination == 'loss' else centers_prev\n",
        "            if self._check_termination(prev):\n",
        "                break\n",
        "            \n",
        "    def _check_termination(self, prev):\n",
        "        if self.termination == 'loss':\n",
        "            return (1 - self.loss/prev) < self.tol\n",
        "        elif self.termination == 'center':\n",
        "            return np.linalg.norm(self.centroids - prev) < self.tol\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def _init_centroids(self):\n",
        "        '''\n",
        "        Kmeans++ initialization\n",
        "        Returns vector of initial centroids\n",
        "        '''\n",
        "        prob = []\n",
        "        total_mass = 0\n",
        "        for m in self.mass_arr:\n",
        "            total_mass += m\n",
        "        for m in self.mass_arr:\n",
        "          prob.append(m / total_mass)\n",
        "        first_idx = np.random.choice(self.n, p = prob)\n",
        "        self.centroids = self.data[first_idx,:]\n",
        "        for kk in range(1,self.k):\n",
        "            P = self._get_selection_prob()\n",
        "            nxt_idx = np.random.choice(self.n,p=P)\n",
        "            self.kpp_inits.add(nxt_idx)\n",
        "            self.centroids = np.vstack([self.centroids,self.data[nxt_idx,:]])\n",
        "\n",
        "    def _get_selection_prob(self):\n",
        "        '''\n",
        "        Outputs vector of selection probabilites\n",
        "        Equal to Distance^2 to nearest centroid\n",
        "        '''\n",
        "        #handle edge case in centroids shape by unsqueezing\n",
        "        if len(self.centroids.shape) == 1:\n",
        "            self.centroids = np.expand_dims(self.centroids, axis=0)\n",
        "\n",
        "        #probability is square distance to closest centroid\n",
        "        D = np.zeros([self.n])\n",
        "        for i in range(self.n):\n",
        "            d = np.linalg.norm(self.data[i,:] - self.centroids, axis=1)\n",
        "            D[i] = np.min(d)\n",
        "        P = []\n",
        "        for i in range(self.n):\n",
        "            P.append(self.mass_arr[i] * (D[i]**2))\n",
        "        P = P / sum(P)\n",
        "        return P  \n",
        "\n",
        "    def _assign_centroids(self):\n",
        "        '''\n",
        "        Computes centroids in Lloyd iterations\n",
        "        '''\n",
        "        self.centroids = np.zeros([self.k,self.d])\n",
        "        c = np.zeros([self.k])\n",
        "        for i in range(self.n):\n",
        "            a = self.assignments[i]\n",
        "            c[a] += self.mass_arr[i]  # weight by mass\n",
        "            self.centroids[a,:] += self.mass_arr[i] * self.data[i,:]   # weight by mass\n",
        "            \n",
        "        for j in range(self.k):\n",
        "            self.centroids[j,:] = self.centroids[j,:] / c[j]\n",
        "\n",
        "        for j in self.empty_clusters: \n",
        "            self._reinit_cluster(j)\n",
        "        self.empty_clusters = []\n",
        "        \n",
        "    def _assign_clusters(self):\n",
        "        '''\n",
        "        Computes clusters in Lloyd iterations\n",
        "        '''\n",
        "        assert (self.k, self.d) == self.centroids.shape, \"Centers wrong shape\"\n",
        "        self.assignments = np.zeros([self.n]).astype(int)\n",
        "        self.loss = 0\n",
        "        for i in range(self.n):\n",
        "            d = np.linalg.norm(self.data[i,:] - self.centroids, axis=1)\n",
        "            d1 = np.linalg.norm(self.data[i,:] - self.centroids, axis=1,ord=1)\n",
        "            self.assignments[i] = int(np.argmin(d))\n",
        "            self.loss += self.mass_arr[i] * (np.min(d)**2)  # weight by mass\n",
        "        self.loss = self.loss / sum(self.mass_arr)\n",
        "        self.empty_clusters = self._check_4_empty_clusters()\n",
        "\n",
        "    def _check_4_empty_clusters(self):\n",
        "        empty_clusters = []\n",
        "        for kappa in range(self.k):\n",
        "            if len(np.where(self.assignments == kappa)[0]) == 0:\n",
        "                empty_clusters.append(kappa)\n",
        "        return empty_clusters\n",
        "\n",
        "    def _reinit_cluster(self, j):\n",
        "        '''\n",
        "        Gets a failed centroid with idx j (empty cluster)\n",
        "        Should replace with new k++ init centroid\n",
        "        in:\n",
        "            j is idx for centroid, 0 <= j <= n\n",
        "        out:\n",
        "            j_prime is idx for next centroid\n",
        "        side-effects:\n",
        "            centroids are update to reflect j -> j'\n",
        "        '''\n",
        "        P = self._get_selection_prob()\n",
        "        j_prime = np.random.choice(self.n,p=P)\n",
        "        self.kpp_inits.add(j_prime)\n",
        "        self.centroids[j,:] = self.data[j_prime,:]\n",
        "        return j_prime\n",
        "\n",
        "\n",
        "class DCnode(Kmeans):\n",
        "    '''\n",
        "    A k-means subproblem for the divide-and-conquer tree\n",
        "    in DC-k-means algorithm\n",
        "    '''\n",
        "    def __init__(self, k, iters):\n",
        "        Kmeans.__init__(self, k, iters=iters)\n",
        "        self.children = []\n",
        "        self.parent = None\n",
        "        self.time = 0\n",
        "        self.loss = 0\n",
        "        self.node_data = set()\n",
        "        self.node_mass = dict() #mass dict\n",
        "        self.mass_arr = []\n",
        "        self.data_prop = set()\n",
        "\n",
        "    def _run_node(self, X):\n",
        "        self._set_node_data(X)\n",
        "        self._lloyd_iterations()\n",
        "\n",
        "    def _set_node_data(self, X):\n",
        "        #print(\"X\", X.shape)\n",
        "        self.data = X[list(self.node_data)]\n",
        "        self.mass_arr = []\n",
        "        for d in self.data:\n",
        "          self.mass_arr.append(self.node_mass[tuple(d)])\n",
        "        self._set_data(self.data)\n",
        "\n",
        "class WeightedDCKmeans():\n",
        "    def __init__(self, ks, widths, iters=10):\n",
        "        '''\n",
        "        Constructor for quantized k-means solved via Lloyd iterations\n",
        "        ks - list of k parameter for each layer of DC-tree\n",
        "        widths - list of width parameter (number of buckets) for each layer\n",
        "        iters - # of iterations to run \n",
        "            (at present, only supports fixed iteration termination)\n",
        "        '''\n",
        "        self.ks = ks\n",
        "        self.widths = widths\n",
        "        self.dc_tree = self._init_tree(ks,widths,iters)\n",
        "        self.data_partition_table = dict()\n",
        "        self.data = dict()\n",
        "        self.mass = dict()  # For mass\n",
        "        self.dels = set()\n",
        "        self.valid_ids = []\n",
        "        self.d = 0\n",
        "        self.n = 0\n",
        "        self.h = len(self.dc_tree)\n",
        "        for i in range(self.h):\n",
        "            self.data[i] = None\n",
        "        # For mass\n",
        "        for i in range(self.h):\n",
        "            self.mass[i] = None\n",
        "            \n",
        "    def run(self, X, assignments=False):\n",
        "        '''\n",
        "        X - numpy matrix, n-by-d, each row is a data point\n",
        "        assignments (optional) - bool flag, computes assignments and loss\n",
        "            NOTE: Without assignments flag, this only returns the centroids\n",
        "        OUTPUT:\n",
        "        centroids - k-by-d matrix of centroids\n",
        "            IF assignments FLAG IS SET ALSO RETURNS:\n",
        "        assignments - Vector of length n, with datapoint to center assignments\n",
        "        loss - The loss of the final partition\n",
        "        '''\n",
        "        self._init_data(X)\n",
        "        self._partition_data(X)\n",
        "        self._run()\n",
        "        if assignments:\n",
        "            assignment_solver = Kmeans(self.ks[0])\n",
        "            assignment_solver._set_data(X)\n",
        "            assignment_solver.centroids = self.centroids\n",
        "            assignment_solver.mass_arr = []\n",
        "            for i in range(len(X)):\n",
        "              assignment_solver.mass_arr.append(1)\n",
        "            assignment_solver._assign_clusters()\n",
        "            self.assignments = assignment_solver.assignments\n",
        "            self.loss = assignment_solver.loss\n",
        "            return self.centroids, self.assignments, self.loss\n",
        "        return self.centroids\n",
        "    \n",
        "    def delete(self, del_idx):\n",
        "        idx = self.valid_ids[del_idx]\n",
        "        self.valid_ids[del_idx] = self.valid_ids.pop()\n",
        "        self.dels.add(idx)\n",
        "        node = self.dc_tree[-1][self.data_partition_table[idx]]\n",
        "        node.node_data.remove(idx)\n",
        "        del node.node_mass[tuple(self.data[self.h-1][idx])]\n",
        "        l = self.h-1\n",
        "        self.n -= 1\n",
        "        while True:\n",
        "            node._run_node(self.data[l])\n",
        "            if node.parent == None:\n",
        "                self.centroids = node.centroids\n",
        "                break\n",
        "            data_prop = list(node.data_prop)\n",
        "            for c_id in range(len(node.centroids)):\n",
        "                idx = data_prop[c_id]\n",
        "                self.data[l][idx] = node.centroids[c_id]\n",
        "            node = node.parent\n",
        "            l -= 1\n",
        "\n",
        "    def _init_data(self,X):\n",
        "        self.n = len(X)\n",
        "        self.valid_ids = list(range(self.n))\n",
        "        self.d = len(X[0])\n",
        "        data_layer_size = self.n\n",
        "        for i in range(self.h-1,-1,-1):\n",
        "            self.data[i] = np.zeros((data_layer_size,self.d))\n",
        "            self.mass[i] = np.zeros((data_layer_size))\n",
        "            data_layer_size = self.ks[i]*self.widths[i] \n",
        "        \n",
        "    def _partition_data(self, X):\n",
        "        self.d = len(X[0])\n",
        "        num_leaves = len(self.dc_tree[-1])\n",
        "        for i in range(len(X)):\n",
        "            leaf_id = np.random.choice(num_leaves)\n",
        "            leaf = self.dc_tree[-1][leaf_id]\n",
        "            self.data_partition_table[i] = leaf_id\n",
        "            leaf.node_data.add(i)\n",
        "            leaf.node_mass[tuple(X[i])] = 1\n",
        "            #print(\"partition: \", tuple(X[i]))\n",
        "            self.data[self.h-1][i] = X[i]\n",
        "\n",
        "    def _run(self):\n",
        "        for l in range(self.h-1,-1,-1):\n",
        "            c = 0\n",
        "            for j in range(self.widths[l]):\n",
        "                subproblem = self.dc_tree[l][j]\n",
        "                subproblem._run_node(self.data[l])\n",
        "                if subproblem.parent == None:\n",
        "                    self.centroids = subproblem.centroids\n",
        "                else:\n",
        "                    for c_id in range(len(subproblem.centroids)):\n",
        "                        subproblem.data_prop.add(c)\n",
        "\n",
        "                        assignment_solver = Kmeans(self.ks[0])\n",
        "                        X = self.data[l]\n",
        "                        data = X[list(subproblem.node_data)]    ###????\n",
        "                        assignment_solver._set_data(data)\n",
        "                        assignment_solver.centroids = subproblem.centroids\n",
        "                        assignment_solver.mass_arr = []\n",
        "                        for i in range(len(data)):\n",
        "                          assignment_solver.mass_arr.append(1)\n",
        "                        assignment_solver._assign_clusters()\n",
        "                        assignments = assignment_solver.assignments\n",
        "                        i = 0\n",
        "                        for assign in assignments:\n",
        "                          if tuple(subproblem.centroids[assign]) not in subproblem.parent.node_mass:\n",
        "                            subproblem.parent.node_mass[tuple(subproblem.centroids[assign])] = subproblem.mass_arr[i]\n",
        "                          else:\n",
        "                            subproblem.parent.node_mass[tuple(subproblem.centroids[assign])] += subproblem.mass_arr[i]\n",
        "                          i += 1\n",
        "\n",
        "                        subproblem.parent.node_data.add(c)\n",
        "                        self.data[l-1][c] = subproblem.centroids[c_id] \n",
        "                        c += 1\n",
        "\n",
        "    def _init_tree(self, ks, widths, iters):\n",
        "        tree = [[DCnode(ks[0],iters)]] # root node\n",
        "        for i in range(1,len(widths)):\n",
        "            k = ks[i]\n",
        "            assert widths[i] % widths[i-1] == 0, \"Inconsistent widths in tree\"\n",
        "            merge_factor = int(widths[i] / widths[i-1])\n",
        "            level = []\n",
        "            for j in range(widths[i-1]):\n",
        "                parent = tree[i-1][j]\n",
        "                for _ in range(merge_factor):\n",
        "                    child = DCnode(k,iters=10)\n",
        "                    child.parent = parent\n",
        "                    parent.children.append(child)\n",
        "                    level.append(child)\n",
        "            tree.append(level)\n",
        "        return tree\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTVUsCw0mVZo",
        "colab_type": "code",
        "outputId": "541b4594-c8ed-46a3-c4b4-585254230662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from del_eff_kmeans import Kmeans, QKmeans, DCKmeans\n",
        "import time\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import silhouette_score, normalized_mutual_info_score\n",
        "from random import sample \n",
        "\n",
        "'''\n",
        "D.2 Datasets\n",
        "• Celltypes [42] consists of 12,009 single cell RNA sequences from a mixture of 4 cell types: microglial cells, endothelial cells, fibroblasts, and mesenchymal stem cells. The data was retrieved from the Mouse Cell Atlas and consists of 10 feature dimensions, reduced from an original 23,433 dimensions using principal component analysis. Such dimensionality reduction procedures are a common practice in computational biology.\n",
        "• Postures [35, 34] consists of 74,975 motion capture recordings of users performing 5 different hand postures with unlabeled markers attached to a left-handed glove.\n",
        "• Covtype [12] consists of 15,120 samples of 52 cartographic variables such as elevation and hillshade shade at various times of day for 7 forest cover types.\n",
        "• Botnet [56] contains statistics summarizing the traffic between different IP addresses for a commercial IoT device (Danmini Doorbell). We aim to distinguish between benign traffic data (49,548 instances) and 11 classes of malicious traffic data from botnet attacks, for a total of 1,018,298 instances.\n",
        "• MNIST [51] consists of 60,000 images of isolated, normalized, handwritten digits. The task is to classify each 28×28 image into one of the ten classes.\n",
        "• Gaussian [#] consists of 5 clusters, each generated from 25-variate Gaussian distribution centered at randomly chosen locations in the unit hypercube. 20,000 samples are taken from each of the 5 clusters, for a total of 100,000 samples. Each Gaussian cluster is spherical with variance of 0.8.\n",
        "  Gaussian consists of 5 clusters, each generated from 25-variate Gaussian distribution\n",
        "  centered at randomly chosen locations in the unit hypercube. 20,000 samples are\n",
        "  taken from each of the 5 clusters, for a total of 100,000 samples. Each Gaussian\n",
        "  cluster is spherical with variance of 0.8.\n",
        "'''\n",
        "'''\n",
        "Celltype (N = 12,009, D = 10, K = 4), \"4celltypes_10pca\"\n",
        "Covtype (N = 15,120, D = 52, K = 7), \"covtype_multiclass\"\n",
        "MNIST (N = 60,000, D = 784, K = 10), \"mnist\"\n",
        "Postures (N = 74,975, D = 15, K = 5), \"postures\"\n",
        "Botnet (N = 1,018,298, D = 115, K = 11), \"bot_attack\" \n",
        "and a synthetic dataset made from a Gaussian mixture model which we call Gaussian (N = 100,000, D = 25, K = 5). \n",
        "'''\n",
        "\n",
        "DATAs = [\"4celltypes_10pca\", \"covtype_multiclass\", \"postures\", \"mnist\", \"bot_attack\"]\n",
        "Ks = [4, 7, 5, 10, 11]\n",
        "mata_loss = {}\n",
        "mata_silcoef = {}\n",
        "mata_nmi = {}\n",
        "mata_runtime = {}\n",
        "mata_traintime = {}\n",
        "for d in DATAs:\n",
        "  # 3 models, each with 5 values\n",
        "  mata_loss[d] = [[] for j in range(3)]\n",
        "  mata_silcoef[d] = [[] for j in range(3)]\n",
        "  mata_nmi[d] = [[] for j in range(3)]\n",
        "  mata_runtime[d] = [[] for j in range(3)]\n",
        "  mata_traintime[d] = [[] for j in range(3)]\n",
        "\n",
        "def show_clustering(centers,assignments,data):\n",
        "  colors = ['r','b','g']\n",
        "  for a in range(10):\n",
        "    data_a = data[assignments == a]\n",
        "    plt.scatter(data_a[:,0],data_a[:,1])\n",
        "    plt.scatter(centers[a,0],centers[a,1],marker='x',color='k')\n",
        "  plt.show()\n",
        "\n",
        "def model_specific_stats(model, features, labels, dataset, m, save, num_deletions):  # m is model index, 0, 1, 2\n",
        "\n",
        "  t0 = time.time()\n",
        "  centers = 0\n",
        "  assignments = 0\n",
        "  loss = 0\n",
        "  if m == 2: \n",
        "    centers, assignments, loss = model.run(features.copy(), assignments=True)\n",
        "  else:\n",
        "    centers, assignments, loss = model.run(features.copy())\n",
        "  t1 = time.time()\n",
        "  print(\"train time: \", t1 - t0)\n",
        "  if save:\n",
        "    mata_traintime[dataset][m].append(t1 - t0)\n",
        "  \n",
        "  # Loss\n",
        "  print('Clustering loss is: ', loss)\n",
        "  if save:\n",
        "    mata_loss[dataset][m].append(loss)\n",
        "\n",
        "  # Silhouette Coefficients\n",
        "  sampled_index = sample(range(labels.shape[0]), 10000)\n",
        "  score = silhouette_score(features[sampled_index], assignments[sampled_index])\n",
        "  print(\"silhouette_score for 10000 random samples: \", score)\n",
        "  if save:\n",
        "    mata_silcoef[dataset][m].append(score)\n",
        "\n",
        "  # Normalized Mutual Information\n",
        "  nmi_score = normalized_mutual_info_score(labels, assignments)\n",
        "  print(\"normalized_mutual_info_score: \", nmi_score)\n",
        "  if save:\n",
        "    mata_nmi[dataset][m].append(nmi_score)\n",
        "\n",
        "  # Deletion runtime\n",
        "  print(\"Online deletion time for \" + str(num_deletions) + \" deletions: \")\n",
        "  t = online_deletion_stream(num_deletions, model)\n",
        "  if save:\n",
        "      mata_runtime[dataset][m].append((t + t1 - t0) / num_deletions)\n",
        "  #print(\"Amortized Runtime to process\" + str(num_deletions) + \"deletions is \", (t + t1 - t0) / num_deletions)\n",
        "\n",
        "def reproduce_result(datasets):\n",
        "  for i in range(5):\n",
        "    print(\"Dataset: \" + DATAs[i])\n",
        "    features = datasets[DATAs[i]][0]\n",
        "    labels = datasets[DATAs[i]][1]\n",
        "    k = Ks[i]\n",
        "    n = datasets[DATAs[i]][1].shape[0]\n",
        "    d = datasets[DATAs[i]][0].shape[1]\n",
        "\n",
        "    # k means model\n",
        "    print(\"k-means model\")\n",
        "    kmeans = Kmeans(k, termination='loss')\n",
        "    model_specific_stats(kmeans, features, labels, DATAs[i], 0)\n",
        "    \n",
        "    # Q-kmeans model\n",
        "    print(\"qkmeans model\")\n",
        "    eps = pow(2, -math.log((n / (k * pow(d, 1.5))), 10) - 3)\n",
        "    qkmeans = QKmeans(k, eps)\n",
        "    model_specific_stats(qkmeans, features, labels, DATAs[i], 1)\n",
        "\n",
        "    # DC-kmeans model\n",
        "    print(\"dc kmeans model\")\n",
        "    w = pow(2, math.ceil(math.log(pow(n, 0.3))/math.log(2)))\n",
        "    print(w)\n",
        "    dckmeans = DCKmeans([k,k],[1,w])\n",
        "    model_specific_stats(dckmeans, features, labels, DATAs[i], 2)\n",
        "\n",
        "def online_deletion_stream(num_dels, model):\n",
        "  t0 = time.time()\n",
        "  c = 1\n",
        "  for _ in range(num_dels):\n",
        "      dr = np.random.choice(model.n,size=1)[0]\n",
        "      #print(f'processing deletion request # {c}...')\n",
        "      model.delete(dr)\n",
        "      c += 1\n",
        "  t = time.time()\n",
        "  print(f'Total time to process {c-1} deletions is {t-t0}')\n",
        "  return t - t0\n",
        "\n",
        "\n",
        "def tune_tree_height(datasets):\n",
        "  for i in set([0, 1, 3]):\n",
        "    print(\"Dataset: \" + DATAs[i])\n",
        "    features = datasets[DATAs[i]][0]\n",
        "    labels = datasets[DATAs[i]][1]\n",
        "    k = Ks[i]\n",
        "    n = datasets[DATAs[i]][1].shape[0]\n",
        "    d = datasets[DATAs[i]][0].shape[1]\n",
        "    \n",
        "    dckmeans = DCKmeans([k, k, k, k],[1, 4, 16, 64])\n",
        "    model_specific_stats(dckmeans, features, labels, DATAs[i], 2, True, 20)\n",
        "\n",
        "    dckmeans = DCKmeans([k, k, k],[1, 8, 64])\n",
        "    model_specific_stats(dckmeans, features, labels, DATAs[i], 2, True, 20)\n",
        "\n",
        "    dckmeans = DCKmeans([k, k],[1, 64])\n",
        "    model_specific_stats(dckmeans, features, labels, DATAs[i], 2, True, 20)\n",
        "\n",
        "\n",
        "def tune_tree_buckets(datasets):\n",
        "    for i in set([0, 1, 3]):\n",
        "      print(\"Dataset: \" + DATAs[i])\n",
        "      features = datasets[DATAs[i]][0]\n",
        "      labels = datasets[DATAs[i]][1]\n",
        "      k = Ks[i]\n",
        "      n = datasets[DATAs[i]][1].shape[0]\n",
        "      d = datasets[DATAs[i]][0].shape[1]\n",
        "      \n",
        "      for j in [2, 3, 4, 5, 6, 7, 8, 9]:\n",
        "        dckmeans = DCKmeans([k, k],[1, 2 ** j])\n",
        "        model_specific_stats(dckmeans, features, labels, DATAs[i], 2, True, 20)\n",
        "\n",
        "def weighted_tree(datasets):\n",
        "  for i in set([0, 1, 2, 3, 4]):\n",
        "      print(\"Dataset: \" + DATAs[i])\n",
        "      features = datasets[DATAs[i]][0]\n",
        "      labels = datasets[DATAs[i]][1]\n",
        "      k = Ks[i]\n",
        "      n = datasets[DATAs[i]][1].shape[0]\n",
        "      d = datasets[DATAs[i]][0].shape[1]\n",
        "      \n",
        "      w = pow(2, math.ceil(math.log(pow(n, 0.3))/math.log(2)))\n",
        "      print(w)\n",
        "      \n",
        "      for j in range(3):\n",
        "        print(\"Weighted\")\n",
        "        w_dckmeans = WeightedDCKmeans([k,k],[1,w])\n",
        "        model_specific_stats(w_dckmeans, features, labels, DATAs[i], 2, True, 1)\n",
        "      \n",
        "      for j in range(3):\n",
        "        print(\"non weighted\")\n",
        "        dckmeans = DCKmeans([k,k],[1,w])\n",
        "        model_specific_stats(dckmeans, features, labels, DATAs[i], 2, True, 1)\n",
        "        \n",
        "def weighted_tree2(datasets):\n",
        "  for i in set([4]):\n",
        "      print(\"Dataset: \" + DATAs[i])\n",
        "      features = datasets[DATAs[i]][0]\n",
        "      labels = datasets[DATAs[i]][1]\n",
        "      k = Ks[i]\n",
        "      n = datasets[DATAs[i]][1].shape[0]\n",
        "      d = datasets[DATAs[i]][0].shape[1]\n",
        "      \n",
        "      w = pow(2, math.ceil(math.log(pow(n, 0.3))/math.log(2)))\n",
        "      print(w)\n",
        "      '''\n",
        "      for j in range(3):\n",
        "        print(\"Weighted\")\n",
        "        w_dckmeans = WeightedDCKmeans([k,k],[1,w])\n",
        "        model_specific_stats(w_dckmeans, features, labels, DATAs[i], 2, True, 1)\n",
        "      '''\n",
        "      for j in range(3):\n",
        "        print(\"non weighted\")\n",
        "        dckmeans = DCKmeans([k,k],[1,w])\n",
        "        model_specific_stats(dckmeans, features, labels, DATAs[i], 2, True, 1)\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  with open(\"/content/drive/My Drive/DeleteEfficient/kmeans_data_deletion_NeurIPS19_datasets_scaled.p\", mode='rb') as f:\n",
        "    datasets = pickle.load(f)\n",
        "\n",
        "  tune_tree_height(datasets)\n",
        "  print(\"loss: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_loss[d])\n",
        "  print(\"\\nsilhouette coef: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_silcoef[d])\n",
        "  print(\"\\nnmi: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_nmi[d])\n",
        "  print(\"\\nruntime: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_runtime[d])\n",
        "  '''  \n",
        "  for i in range(5):\n",
        "    reproduce_result(datasets)\n",
        "  \n",
        "  print(\"loss: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_loss[d])\n",
        "  print(\"\\nsilhouette coef: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_silcoef[d])\n",
        "  print(\"\\nnmi: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_nmi[d])\n",
        "  print(\"\\nruntime: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_runtime[d])\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  tune_tree_buckets(datasets)\n",
        "  print(\"loss: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_loss[d])\n",
        "  print(\"\\nsilhouette coef: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_silcoef[d])\n",
        "  print(\"\\nnmi: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_nmi[d])\n",
        "  print(\"\\nruntime: \")\n",
        "  for d in DATAs:\n",
        "    print(d, mata_runtime[d])\n",
        "  \n",
        "  for i in range(len(DATAs)):\n",
        "    n = datasets[DATAs[i]][1].shape[0]\n",
        "    w = pow(2, math.ceil(math.log(pow(n, 0.3))/math.log(2)))\n",
        "    print(w)\n",
        "  '''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 4celltypes_10pca\n",
            "train time:  4.541198968887329\n",
            "Clustering loss is:  0.07043462345989038\n",
            "silhouette_score for 10000 random samples:  0.6466446618121309\n",
            "normalized_mutual_info_score:  0.052616379980917234\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 1.4788517951965332\n",
            "train time:  3.947449207305908\n",
            "Clustering loss is:  0.07708804663717818\n",
            "silhouette_score for 10000 random samples:  0.6422856006930893\n",
            "normalized_mutual_info_score:  0.038441320597657966\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 1.5109143257141113\n",
            "train time:  3.938832998275757\n",
            "Clustering loss is:  0.02371198958964685\n",
            "silhouette_score for 10000 random samples:  0.5316485837472027\n",
            "normalized_mutual_info_score:  0.20987959737194256\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 2.6517980098724365\n",
            "Dataset: covtype_multiclass\n",
            "train time:  5.909093618392944\n",
            "Clustering loss is:  1.105109404189058\n",
            "silhouette_score for 10000 random samples:  0.23095893986408836\n",
            "normalized_mutual_info_score:  0.35364759747669544\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 2.3139781951904297\n",
            "train time:  5.9215474128723145\n",
            "Clustering loss is:  1.139726344532491\n",
            "silhouette_score for 10000 random samples:  0.23200795869542062\n",
            "normalized_mutual_info_score:  0.31681786957500724\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 2.523130416870117\n",
            "train time:  5.89788293838501\n",
            "Clustering loss is:  0.9865279847263123\n",
            "silhouette_score for 10000 random samples:  0.19559611248273953\n",
            "normalized_mutual_info_score:  0.3402574839860663\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 4.721892595291138\n",
            "Dataset: mnist\n",
            "train time:  40.56819820404053\n",
            "Clustering loss is:  40.31207756539614\n",
            "silhouette_score for 10000 random samples:  0.07241111107344628\n",
            "normalized_mutual_info_score:  0.4855573014135641\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 13.1165771484375\n",
            "train time:  40.525975942611694\n",
            "Clustering loss is:  40.09476200474456\n",
            "silhouette_score for 10000 random samples:  0.06784065529451926\n",
            "normalized_mutual_info_score:  0.4871695718759179\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 13.765998601913452\n",
            "train time:  40.24881434440613\n",
            "Clustering loss is:  39.84932643107801\n",
            "silhouette_score for 10000 random samples:  0.07439360937836823\n",
            "normalized_mutual_info_score:  0.4993270208776445\n",
            "Online deletion time for 20 deletions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total time to process 20 deletions is 19.649147987365723\n",
            "loss: \n",
            "4celltypes_10pca [[], [], [0.07043462345989038, 0.07708804663717818, 0.02371198958964685]]\n",
            "covtype_multiclass [[], [], [1.105109404189058, 1.139726344532491, 0.9865279847263123]]\n",
            "postures [[], [], []]\n",
            "mnist [[], [], [40.31207756539614, 40.09476200474456, 39.84932643107801]]\n",
            "bot_attack [[], [], []]\n",
            "\n",
            "silhouette coef: \n",
            "4celltypes_10pca [[], [], [0.6466446618121309, 0.6422856006930893, 0.5316485837472027]]\n",
            "covtype_multiclass [[], [], [0.23095893986408836, 0.23200795869542062, 0.19559611248273953]]\n",
            "postures [[], [], []]\n",
            "mnist [[], [], [0.07241111107344628, 0.06784065529451926, 0.07439360937836823]]\n",
            "bot_attack [[], [], []]\n",
            "\n",
            "nmi: \n",
            "4celltypes_10pca [[], [], [0.052616379980917234, 0.038441320597657966, 0.20987959737194256]]\n",
            "covtype_multiclass [[], [], [0.35364759747669544, 0.31681786957500724, 0.3402574839860663]]\n",
            "postures [[], [], []]\n",
            "mnist [[], [], [0.4855573014135641, 0.4871695718759179, 0.4993270208776445]]\n",
            "bot_attack [[], [], []]\n",
            "\n",
            "runtime: \n",
            "4celltypes_10pca [[], [], [0.3010025382041931, 0.272918176651001, 0.32953155040740967]]\n",
            "covtype_multiclass [[], [], [0.4111535906791687, 0.4222338914871216, 0.5309887766838074]]\n",
            "postures [[], [], []]\n",
            "mnist [[], [], [2.6842387676239015, 2.7145987272262575, 2.9948981165885926]]\n",
            "bot_attack [[], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfoWgo7H-7_a",
        "colab_type": "code",
        "outputId": "20560303-aaa0-4548-a7ca-d354baaa32af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (12009, 10)\n",
            "X (90, 10)\n",
            "X (90, 10)\n",
            "X (90, 10)\n",
            "X (30, 10)\n",
            "train time:  13.523385763168335\n",
            "Clustering loss is:  0.009875495187645595\n",
            "silhouette_score for 10000 random samples:  0.3161159931897116\n",
            "normalized_mutual_info_score:  0.3696796506454962\n",
            "Online deletion time for 100 deletions: \n",
            "processing deletion request # 1...\n",
            "X (12009, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X (90, 10)\n",
            "X (30, 10)\n",
            "processing deletion request # 2...\n",
            "X (12009, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9a14b21c760f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    421\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/DeleteEfficient/kmeans_data_deletion_NeurIPS19_datasets_scaled.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-9a14b21c760f>\u001b[0m in \u001b[0;36mtune\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"4celltypes_10pca\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0mdckmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCKmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m   \u001b[0mmodel_specific_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdckmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9a14b21c760f>\u001b[0m in \u001b[0;36mmodel_specific_stats\u001b[0;34m(model, features, labels, dataset, m, save)\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[0;31m# Deletion runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Online deletion time for 100 deletions: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_deletion_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mmata_runtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-e3f42008049e>\u001b[0m in \u001b[0;36monline_deletion_stream\u001b[0;34m(num_dels, model)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'processing deletion request # {c}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m       \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9a14b21c760f>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, del_idx)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9a14b21c760f>\u001b[0m in \u001b[0;36m_run_node\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_node_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lloyd_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9a14b21c760f>\u001b[0m in \u001b[0;36m_set_node_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmass_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmass_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_mass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0.03875306485028992, 0.9508651057926513, 0.14521251535453514, 0.22925330579899278, 0.1486806430960327, 0.7280478592339716, 0.5088348926419245, 0.2264354970140198, 0.5089793109301216, 0.5970703811151891)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4OcJODu4gVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = [2, 3, 4, 5, 6, 7, 8, 9]\n",
        "nmi1 = [0.3566097971284424, 0.31829612599551427, 0.26160267063730136, 0.2305281490495208, 0.19757822958459795, 0.2033565054065221, 0.29498712519888504, 0.28831689847928715]\n",
        "nmi2 = [0.3425311624096031, 0.3094292442236454, 0.35199981877611436, 0.3318195994281116, 0.3293841491553029, 0.33256364404326855, 0.32973415740883605, 0.3547205599550721]\n",
        "nmi3 = [0.48118463715576776, 0.43701346398934654, 0.4907510934535729, 0.49129394972671414, 0.4969504988136086, 0.473456792609371, 0.4853471975095277, 0.4639135028397989]\n",
        "plt.xlabel('Tree width (number of nodes in the second layer)')\n",
        "plt.ylabel('Normalized Mutual Information score')\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Tree width (number of nodes in the second layer)')\n",
        "plt.ylabel('Silhouette Coefficient')\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Tree width (number of nodes in the second layer)')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Tree width (number of nodes in the second layer)')\n",
        "plt.ylabel('Amortized Runtime')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "loss: \n",
        "4celltypes_10pca [[], [], [0.018650796115511764, 0.021989759573573724, 0.02123363641761834, 0.027181128695309625, 0.025302743263513207, 0.022640554885324852, 0.02343571199078768, 0.02120072788214959]]\n",
        "covtype_multiclass [[], [], [1.0093872899568472, 1.0152784877894863, 0.9924148811850896, 0.9681774327169707, 1.039034620146096, 1.0359270546129151, 0.9939269616770611, 1.0239888286047558]]\n",
        "postures [[], [], []]\n",
        "mnist [[], [], [39.6887819207228, 40.85613631974319, 40.033193525386665, 39.61676658453061, 39.824881151451414, 40.029429842563395, 39.98630161691774, 40.603515379831684]]\n",
        "bot_attack [[], [], []]\n",
        "\n",
        "silhouette coef: \n",
        "4celltypes_10pca [[], [], [0.38458809948056094, 0.3255203098801613, 0.4061978235197673, 0.4920635885266285, 0.5391688081505716, 0.5333195615716323, 0.4128338180162358, 0.41701331060616875]]\n",
        "covtype_multiclass [[], [], [0.2192693277128411, 0.21265828300708067, 0.19295527681699381, 0.28230270768701393, 0.2193945966633595, 0.2204125519130182, 0.188395444095423, 0.23985626611193028]]\n",
        "postures [[], [], []]\n",
        "mnist [[], [], [0.06841322262967865, 0.05969255745734885, 0.06913850133189553, 0.05874127371162567, 0.06744032478794254, 0.07063882723256279, 0.075351388420552, 0.07440175793721708]]\n",
        "bot_attack [[], [], []]\n",
        "\n",
        "nmi: \n",
        "4celltypes_10pca [[], [], [0.3566097971284424, 0.31829612599551427, 0.26160267063730136, 0.2305281490495208, 0.19757822958459795, 0.2033565054065221, 0.29498712519888504, 0.28831689847928715]]\n",
        "covtype_multiclass [[], [], [0.3425311624096031, 0.3094292442236454, 0.35199981877611436, 0.3318195994281116, 0.3293841491553029, 0.33256364404326855, 0.32973415740883605, 0.3547205599550721]]\n",
        "postures [[], [], []]\n",
        "mnist [[], [], [0.48118463715576776, 0.43701346398934654, 0.4907510934535729, 0.49129394972671414, 0.4969504988136086, 0.473456792609371, 0.4853471975095277, 0.4639135028397989]]\n",
        "bot_attack [[], [], []]\n",
        "\n",
        "runtime: \n",
        "4celltypes_10pca [[], [], [13.825237035751343, 6.947296857833862, 3.735635995864868, 2.4677200317382812, 2.105858087539673, 3.104292154312134, 5.404259920120239, 10.086616039276123]]\n",
        "covtype_multiclass [[], [], [21.948658227920532, 10.81125783920288, 5.813812017440796, 4.123564958572388, 3.8815248012542725, 5.878916263580322, 10.245152950286865, 20.703763961791992]]\n",
        "postures [[], [], []]\n",
        "mnist [[], [], [176.21602416038513, 85.9961142539978, 44.67074203491211, 26.727442026138306, 18.86534309387207, 20.994234085083008, 34.139708280563354, 63.156938791275024]]\n",
        "bot_attack [[], [], []]\n",
        "\n",
        "traintime: \n",
        "4celltypes_10pca [[], [], [3.0344059467315674, 3.032461166381836, 2.9939041137695312, 3.106005907058716, 3.138749837875366, 3.425178050994873, 3.518235206604004, 4.029762029647827]]\n",
        "covtype_multiclass [[], [], [4.795390844345093, 4.78167200088501, 4.499297142028809, 4.909643173217773, 5.128259897232056, 4.680608749389648, 5.149761199951172, 5.842344045639038]]\n",
        "postures [[], [], []]\n",
        "mnist [[], [], [37.85356378555298, 36.322457790374756, 37.79789113998413, 36.96315813064575, 39.328386068344116, 39.643174171447754, 39.99007725715637, 42.39094400405884]]\n",
        "bot_attack [[], [], []]\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "x = np.arange(10)\n",
        "\n",
        "plt.plot(x, x)\n",
        "plt.plot(x, 2 * x)\n",
        "plt.plot(x, 3 * x)\n",
        "plt.plot(x, 4 * x)\n",
        "\n",
        "plt.legend(['y = x', 'y = 2x', 'y = 3x', 'y = 4x'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qKSe1I6vyi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}